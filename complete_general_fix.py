#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COMPLETE GENERAL CATEGORY FIX
Combines ALL fix dictionaries for 100% coverage of 414 cards
"""

import csv
import sys

# Import all fix dictionaries
from fix_general_comprehensive import fixes as main_fixes
from additional_fixes import additional_fixes
from final_12_fixes import final_12_fixes

# Merge ALL fixes
all_fixes = {**main_fixes, **additional_fixes, **final_12_fixes}

# Mapping for truncated Spanish entries
truncated_mapping = {
    "Ay pap√°mam√°": "Ay pap√°/mam√°",
    "Debe ser ilegal ser tan lindalindo": "Debe ser ilegal ser tan linda/lindo",
    "DiablaDiablo": "Diabla/Diablo",
    "El cielo est√° de luto porque perdi√≥ una": "El cielo est√° de luto porque perdi√≥ una estrella",
    "Eres como el aguardiente me subes direct": "Eres como el aguardiente me subes directo",
    "Eres la raz√≥n por la que el caf√© colombi": "Eres la raz√≥n por la que el caf√© colombiano es tan bueno",
    "Eres m√≠om√≠a": "Eres m√≠o/m√≠a",
    "Eres una diosadios": "Eres una diosa/dios",
    "Eres √∫nico√∫nica": "Eres √∫nico/√∫nica",
    "Hice algo para que te emberracaras conmi": "Hice algo para que te emberracaras conmigo",
    "Me derrites m√°s r√°pido que el queso en u": "Me derrites m√°s r√°pido que el queso en una arepa",
    "Me vuelves locoloca": "Me vuelves loco/loca",
    "Necesito un doctordoctora": "Necesito un doctor/doctora",
    "Perd√≠ mi n√∫mero de tel√©fono me das el tu": "Perd√≠ mi n√∫mero de tel√©fono me das el tuyo",
    "Polic√≠a te arrest√≥ por estar tan buenobu": "Polic√≠a te arrest√≥ por estar tan bueno/buena",
    "Qued√© muertomuerta": "Qued√© muerto/muerta",
    "Quisiera ser tu almohada para estar en t": "Quisiera ser tu almohada para estar en tus sue√±os",
    "Solo m√≠om√≠a": "Solo m√≠o/m√≠a",
    "Viejovieja": "Viejo/vieja",
    "√ëero√±era": "√ëero/√±era",
}

def process_complete():
    """Process ALL general category cards with complete fixes"""
    input_file = '/tmp/general_cards.csv'
    output_file = '/Users/biobook/Projects/anki/colombian_spanish/spanish-srs/GENERAL_COMPLETE_FIXES.csv'

    with open(input_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        fieldnames = reader.fieldnames
        cards = list(reader)

    # Track statistics
    fixed_count = 0
    already_complete = 0
    not_found = []

    # Apply fixes to ALL cards
    for card in cards:
        front = card['Front (ES)'].strip()

        # Check if truncated, map to full version
        full_front = truncated_mapping.get(front, front)

        # Try to find fix using exact match first, then truncated version
        fix_data = None
        if full_front in all_fixes:
            fix_data = all_fixes[full_front]
        elif front in all_fixes:  # Also try original truncated
            fix_data = all_fixes[front]

        if fix_data:
            # Apply translation
            card['Back (EN)'] = fix_data['back']

            # Apply cultural context to Notes field
            card['Notes'] = fix_data['notes']

            # Use Alternative field for additional context if empty
            if not card['Alternative'].strip():
                card['Alternative'] = fix_data.get('alt_example', '')

            fixed_count += 1
        else:
            # Check if already has complete data
            back = card['Back (EN)'].strip()
            notes = card['Notes'].strip()

            is_complete = (
                back and
                not back.startswith('[') and
                not back.startswith('I eat...') and
                not back.startswith('How...') and
                not back.startswith('Who...') and
                not back.startswith('What...') and
                not back.startswith('I don') and
                not back.startswith('Dude...') and
                not back.startswith('Traffic jam...') and
                not back.startswith('Later...') and
                notes
            )

            if is_complete:
                already_complete += 1
            else:
                not_found.append(front)

    # Write output
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(cards)

    # Print statistics
    print(f"\n{'='*70}")
    print(f"üéØ GENERAL CATEGORY - FINAL COMPLETE FIX REPORT")
    print(f"{'='*70}")
    print(f"Total general category cards:  {len(cards)}")
    print(f"Cards fixed in this run:       {fixed_count}")
    print(f"Cards already complete:        {already_complete}")
    print(f"Cards still incomplete:        {len(not_found)}")
    print(f"\n‚úÖ Fix coverage: {(fixed_count + already_complete) / len(cards) * 100:.1f}%")

    if not_found:
        print(f"\n‚ö†Ô∏è  Still need attention ({len(not_found)} cards):")
        for i, card in enumerate(not_found, 1):
            print(f"  {i}. {card}")

    print(f"\nüìÑ Output written to:")
    print(f"   {output_file}")
    print(f"{'='*70}\n")

    # Success summary
    if len(not_found) == 0:
        print("üéâ SUCCESS! All 414 general category cards now have:")
        print("   ‚úì Complete translations")
        print("   ‚úì Cultural context in Notes")
        print("   ‚úì Example usage in Alternative")
        print()

    return fixed_count, already_complete, len(not_found)

if __name__ == '__main__':
    fixed, complete, remaining = process_complete()
    sys.exit(0 if remaining == 0 else 1)
